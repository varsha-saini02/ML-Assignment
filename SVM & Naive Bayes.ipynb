{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **THEORY QUESTIONS**"],"metadata":{"id":"YS5WtwsAJZsM"}},{"cell_type":"markdown","source":["# Question 1:  What is a Support Vector Machine (SVM), and how does it work?\n","\n","###Answers:\n","\n","### What is an SVM?\n","\n","Support Vector Machine (SVM) is a powerful supervised machine learning algorithm primarily used for **classification**, though it can also handle regression tasks. Its main goal is to find the best boundary (called a **hyperplane**) that separates different classes in your data.\n","\n","Think of it like drawing a line on a 2D plot to separate red dots from blue dots—but it works in much higher dimensions too.\n","\n","### How does it work?\n","\n","1. **Finding the Best Separating Hyperplane**\n","   Imagine you have two classes of data points scattered on a graph. SVM tries to find a line (in 2D) or a hyperplane (in higher dimensions) that splits these classes apart.\n","\n","2. **Maximizing the Margin**\n","   Instead of just any line that separates the classes, SVM picks the one that maximizes the **margin** — the distance between the closest points of each class to the hyperplane.\n","   Those closest points are called **support vectors** because they \"support\" or define the position of the boundary.\n","\n","3. **Handling Non-linearly Separable Data**\n","   What if you can’t draw a straight line to separate the classes? SVM cleverly uses something called a **kernel trick** — a mathematical function that transforms your data into a higher-dimensional space where it *is* linearly separable.\n","   Common kernels include:\n","\n","   * Linear\n","   * Polynomial\n","   * Radial Basis Function (RBF)\n","\n","4. **Soft Margin for Noisy Data**\n","   Real-world data is messy and might overlap. SVM allows some points to be on the wrong side of the margin with a **soft margin** parameter (often called C), balancing margin size and classification errors.\n","\n","### Why is SVM powerful?\n","\n","* Works well in high-dimensional spaces.\n","* Effective even when the number of features exceeds the number of samples.\n","* Robust against overfitting due to maximizing margin.\n","* Versatile through kernels for nonlinear data.\n","\n","---\n","\n","# Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n","\n","###Answer:\n","\n","The difference between **Hard Margin** and **Soft Margin** SVM is pretty central to how SVM handles data — especially when the data isn’t perfectly clean or linearly separable.\n","\n","Here’s the lowdown:\n","\n","### Hard Margin SVM\n","\n","* **What it is:**\n","  Hard Margin SVM tries to find a **perfectly clean** hyperplane that **strictly separates** the two classes with no errors at all.\n","\n","* **Key idea:**\n","  The margin must be **wide**, and **no data points** are allowed to lie inside the margin or on the wrong side of the boundary.\n","\n","* **When it works:**\n","  Only when the data is **linearly separable** without any overlap or noise.\n","\n","* **Limitations:**\n","\n","  * Doesn’t tolerate misclassified points or noise.\n","  * Not practical for most real-world datasets where classes overlap or have outliers.\n","\n","* **Mathematically:**\n","  The optimization requires all points to satisfy:\n","\n","  $$\n","  y_i (\\mathbf{w} \\cdot \\mathbf{x_i} + b) \\geq 1\n","  $$\n","\n","  with no violations allowed.\n","\n","### Soft Margin SVM\n","\n","* **What it is:**\n","  Soft Margin SVM allows some **flexibility** by tolerating some misclassifications or points inside the margin.\n","\n","* **Key idea:**\n","  It tries to find a hyperplane that balances **maximizing the margin** and **minimizing classification errors**.\n","\n","* **How it does this:**\n","  Introduces **slack variables** ($\\xi_i$) that measure how much each point violates the margin.\n","\n","* **Trade-off controlled by parameter $C$:**\n","\n","  * A **large $C$** means \"penalize errors heavily\" — so fewer misclassifications but possibly a smaller margin (closer to hard margin).\n","  * A **small $C$** means \"allow more errors\" to get a wider margin (better generalization, often).\n","\n","* **When it works:**\n","  Best for **noisy or overlapping data** where perfect separation isn’t possible.\n","\n","* **Mathematically:**\n","  The optimization becomes:\n","\n","  $$\n","  \\min \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum \\xi_i\n","  $$\n","\n","  subject to:\n","\n","  $$\n","  y_i (\\mathbf{w} \\cdot \\mathbf{x_i} + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n","  $$\n","\n","### In simple terms:\n","\n","| Feature            | Hard Margin                        | Soft Margin                          |\n","| ------------------ | ---------------------------------- | ------------------------------------ |\n","| Error allowed?     | No, zero misclassification allowed | Yes, allows some misclassifications  |\n","| Data type          | Perfectly separable                | Overlapping/noisy data               |\n","| Margin flexibility | Fixed, strict                      | Flexible, balances margin and errors |\n","| Use case           | Rare in real world                 | Most practical and commonly used SVM |\n","\n","---\n","\n","# Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n","\n","###Answer:\n","\n","### What is the Kernel Trick?\n","\n","The kernel trick is a clever mathematical shortcut that lets SVMs **handle non-linearly separable data** without explicitly transforming it into higher dimensions.\n","\n","### Why is this needed?\n","\n","Imagine your data points can’t be separated by a straight line in 2D, but if you were to “lift” them into 3D space, they **would** be separable by a flat plane. The problem is, explicitly calculating those new coordinates in high-dimensional space is often computationally expensive or impossible.\n","\n","The kernel trick **avoids the heavy lifting** by computing the *dot product* between points as if they were already transformed into that high-dimensional space — **without actually calculating their coordinates in that space**.\n","\n","### How it works:\n","\n","* SVM decision functions rely on **dot products** between feature vectors.\n","* A kernel function $K(x, y)$ computes this dot product **in some transformed feature space**.\n","* By plugging in the kernel function, SVM can operate as if the data were mapped into a higher-dimensional space, **enabling non-linear separation**.\n","\n","### Example Kernel: Radial Basis Function (RBF) Kernel\n","\n","**Formula:**\n","\n","$$\n","K(x, x') = \\exp\\left(-\\gamma \\|x - x'\\|^2\\right)\n","$$\n","\n","* Here, $\\gamma$ is a parameter controlling the influence range of a single training example.\n","* It measures similarity based on distance: points closer together have a higher kernel value (near 1), and points far apart have a value closer to 0.\n","\n","### Use Case for RBF Kernel:\n","\n","* When your data is **not linearly separable** but forms clusters or complex boundaries.\n","* For example, imagine data points arranged in concentric circles or blobs that can’t be separated with a straight line — RBF helps map them into a space where a linear separator works.\n","* It’s by far the **most popular kernel** because of its flexibility and ability to model complex patterns without overfitting if parameters are tuned well.\n","\n","### Quick recap:\n","\n","* **Kernel trick:** Calculate similarity in a high-dimensional space without explicitly transforming data.\n","* **RBF kernel:** Measures how close points are, enabling flexible non-linear boundaries.\n","* Perfect for complex datasets where linear SVM fails.\n","---\n","\n","# Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n","\n","###Answer:\n","\n","Naïve Bayes is a **probabilistic machine learning algorithm** used mainly for classification tasks. It’s based on **Bayes’ Theorem**, which helps you update the probability estimate for a hypothesis as more evidence comes in.\n","\n","In simple terms, it predicts the class of a data point by calculating the probability that it belongs to each class and then picking the class with the highest probability.\n","\n","### How does it work?\n","\n","* It looks at the features (attributes) of your data.\n","* Uses Bayes’ theorem to calculate the probability of the data belonging to each class.\n","* Assigns the class with the **highest posterior probability**.\n","\n","### Why is it called “Naïve”?\n","\n","The “naïve” part comes from a **strong assumption it makes**:\n","\n","* It assumes **all features are independent of each other**, given the class label.\n","* In reality, features often influence each other (they’re correlated), but Naïve Bayes ignores this and treats each feature as if it stands alone.\n","\n","This assumption is what makes it “naïve” — it simplifies the math drastically, making the algorithm fast and efficient, even if the assumption isn’t perfectly true.\n","\n","### Why does it still work well?\n","\n","Surprisingly, even when the independence assumption is violated, Naïve Bayes often performs **really well** in practice, especially for:\n","\n","* Text classification (like spam filtering or sentiment analysis)\n","* Medical diagnosis\n","* Any problem where the independence assumption is “close enough”\n","\n","### Quick Bayes Theorem refresher:\n","\n","$$\n","P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)}\n","$$\n","\n","* $P(C | X)$ = probability of class $C$ given data $X$ (what we want)\n","* $P(X | C)$ = probability of data $X$ given class $C$\n","* $P(C)$ = prior probability of class $C$\n","* $P(X)$ = probability of data $X$ (normalizing constant)\n","---\n","\n","# Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n","\n","###Answer:\n","\n","### 1. Gaussian Naïve Bayes\n","\n","* **What it assumes:**\n","  The features follow a **normal (Gaussian) distribution**.\n","\n","* **How it works:**\n","  For each feature and class, it estimates the mean and variance, then calculates the likelihood using the Gaussian probability density function.\n","\n","* **Use case:**\n","  When your features are **continuous numerical data** (like height, weight, temperature, or any real-valued measurements).\n","\n","* **Example:**\n","  Predicting if a patient has a disease based on continuous lab test results.\n","\n","### 2. Multinomial Naïve Bayes\n","\n","* **What it assumes:**\n","  Features represent **counts or frequencies** (non-negative integers).\n","\n","* **How it works:**\n","  It models the probability of the features as a **multinomial distribution** — basically, how often each feature occurs.\n","\n","* **Use case:**\n","  Commonly used in **text classification**, where features are **word counts** or frequencies in documents.\n","\n","* **Example:**\n","  Spam detection or sentiment analysis based on how often certain words appear in emails or reviews.\n","\n","### 3. Bernoulli Naïve Bayes\n","\n","* **What it assumes:**\n","  Features are **binary (0/1)** — indicating presence or absence.\n","\n","* **How it works:**\n","  It models features with a **Bernoulli distribution**, focusing on whether a feature is present or not in a sample.\n","\n","* **Use case:**\n","  When your features are **binary indicators**, such as whether a word occurs or doesn’t in a document (ignoring how many times it appears).\n","\n","* **Example:**\n","  Document classification where you only care if a word is present, not its frequency.\n","\n","### Quick comparison summary:\n","\n","| Variant            | Data type             | Distribution Assumed | Typical Use Case                                            |\n","| ------------------ | --------------------- | -------------------- | ----------------------------------------------------------- |\n","| **Gaussian NB**    | Continuous numerical  | Gaussian (Normal)    | Numeric measurements                                        |\n","| **Multinomial NB** | Count data (integers) | Multinomial          | Text classification with word counts                        |\n","| **Bernoulli NB**   | Binary (0/1) features | Bernoulli (binary)   | Text classification with binary features (presence/absence) |\n","\n","### When to pick which?\n","\n","* If your features are **continuous real numbers**, go with **Gaussian NB**.\n","* If your features are **counts or frequencies**, pick **Multinomial NB**.\n","* If your features are **binary indicators**, choose **Bernoulli NB**.\n","---"],"metadata":{"id":"3nlgUovfJfuY"}},{"cell_type":"markdown","source":["## **PRACTICAL QUESTIONS**"],"metadata":{"id":"AY-GWxPTyPPD"}},{"cell_type":"markdown","source":["# Question 6:   Write a Python program to:\n","\n","  ● Load the Iris dataset\n","\n","  ● Train an SVM Classifier with a linear kernel\n","  \n","  ● Print the model's accuracy and support vectors."],"metadata":{"id":"6GBrN_ULMs5h"}},{"cell_type":"code","source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split dataset into train and test (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train SVM with linear kernel\n","svm_clf = SVC(kernel='linear', random_state=42)\n","svm_clf.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = svm_clf.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n","\n","# Print support vectors\n","print(\"Support Vectors:\")\n","print(svm_clf.support_vectors_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BVi-h6PNDAD","outputId":"625c2fb2-a8c1-4041-d01c-96425f21ec8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.00\n","Support Vectors:\n","[[4.8 3.4 1.9 0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [4.5 2.3 1.3 0.3]\n"," [5.6 3.  4.5 1.5]\n"," [5.4 3.  4.5 1.5]\n"," [6.7 3.  5.  1.7]\n"," [5.9 3.2 4.8 1.8]\n"," [5.1 2.5 3.  1.1]\n"," [6.  2.7 5.1 1.6]\n"," [6.3 2.5 4.9 1.5]\n"," [6.1 2.9 4.7 1.4]\n"," [6.5 2.8 4.6 1.5]\n"," [6.9 3.1 4.9 1.5]\n"," [6.3 2.3 4.4 1.3]\n"," [6.3 2.5 5.  1.9]\n"," [6.3 2.8 5.1 1.5]\n"," [6.3 2.7 4.9 1.8]\n"," [6.  3.  4.8 1.8]\n"," [6.  2.2 5.  1.5]\n"," [6.2 2.8 4.8 1.8]\n"," [6.5 3.  5.2 2. ]\n"," [7.2 3.  5.8 1.6]\n"," [5.6 2.8 4.9 2. ]\n"," [5.9 3.  5.1 1.8]\n"," [4.9 2.5 4.5 1.7]]\n"]}]},{"cell_type":"markdown","source":["# Question 7:  Write a Python program to:\n","\n","  ● Load the Breast Cancer dataset\n","\n","  ● Train a Gaussian Naïve Bayes model\n","  \n","  ● Print its classification report including precision, recall, and F1-score."],"metadata":{"id":"d-vjWqS0NLWz"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import classification_report\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train Gaussian Naive Bayes model\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = gnb.predict(X_test)\n","\n","# Print classification report\n","report = classification_report(y_test, y_pred, target_names=data.target_names)\n","print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilryU5UeND_5","outputId":"8c1c95aa-9a27-48d2-998f-b4c5a2e28f2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   malignant       1.00      0.93      0.96        43\n","      benign       0.96      1.00      0.98        71\n","\n","    accuracy                           0.97       114\n","   macro avg       0.98      0.97      0.97       114\n","weighted avg       0.97      0.97      0.97       114\n","\n"]}]},{"cell_type":"markdown","source":["# Question 8: Write a Python program to:\n","\n","  ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best `C` and `gamma`.\n","\n","  ● Print the best hyperparameters and accuracy."],"metadata":{"id":"PVn6FS4dNdD0"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load Wine dataset\n","data = load_wine()\n","X = data.data\n","y = data.target\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define SVM model\n","svm = SVC()\n","\n","# Hyperparameter grid\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n","    'kernel': ['rbf']  # Using RBF kernel to include gamma parameter\n","}\n","\n","# Setup GridSearchCV\n","grid_search = GridSearchCV(svm, param_grid, cv=5, n_jobs=-1, verbose=1)\n","\n","# Train with hyperparameter tuning\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters\n","print(\"Best Hyperparameters:\", grid_search.best_params_)\n","\n","# Evaluate on test data\n","y_pred = grid_search.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Test Set Accuracy: {accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugBPmlHlNYxr","outputId":"f62ec9c6-83b3-4cfb-ea58-9141ad17ae08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 24 candidates, totalling 120 fits\n","Best Hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n","Test Set Accuracy: 0.83\n"]}]},{"cell_type":"markdown","source":["# Question 9: Write a Python program to:\n","\n","  ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using `sklearn.datasets.fetch_20newsgroups`).\n","\n","  ● Print the model's ROC-AUC score for its predictions."],"metadata":{"id":"twvqWzlZN15g"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import label_binarize\n","from sklearn.metrics import roc_auc_score\n","from sklearn.multiclass import OneVsRestClassifier\n","import numpy as np\n","\n","# Load subset of 20 Newsgroups dataset (to keep it manageable)\n","categories = ['alt.atheism', 'comp.graphics', 'sci.med']\n","newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n","\n","X = newsgroups.data\n","y = newsgroups.target\n","\n","# Convert text to feature vectors (word counts)\n","vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n","X_vect = vectorizer.fit_transform(X)\n","\n","# Binarize labels for multi-class ROC-AUC\n","y_bin = label_binarize(y, classes=np.arange(len(categories)))\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_vect, y_bin, test_size=0.2, random_state=42)\n","\n","# Use OneVsRest wrapper since roc_auc_score requires binary format for multi-class\n","clf = OneVsRestClassifier(MultinomialNB())\n","clf.fit(X_train, y_train)\n","\n","# Predict probabilities\n","y_proba = clf.predict_proba(X_test)\n","\n","# Calculate ROC-AUC score (macro average across classes)\n","roc_auc = roc_auc_score(y_test, y_proba, average='macro')\n","\n","print(f\"ROC-AUC Score (macro-average): {roc_auc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rdg06KUkNv5m","outputId":"58a37ae0-1ebc-4f12-b63c-a7bdcb8e0bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC-AUC Score (macro-average): 0.993\n"]}]},{"cell_type":"markdown","source":["# Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n","\n","Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n","\n","  ● Text with diverse vocabulary\n","\n","  ● Potential class imbalance (far more legitimate emails than spam)\n","\n","  ● Some incomplete or missing data\n","\n","Explain the approach you would take to:\n","\n","  ● Preprocess the data (e.g. text vectorization, handling missing data)\n","\n","  ● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n","\n","  ● Address class imbalance\n","\n","  ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution."],"metadata":{"id":"Bss7Dn6MOaQQ"}},{"cell_type":"markdown","source":["###Answer:\n","\n","## 1. **Data Preprocessing**\n","\n","### Handling Text Data\n","\n","* **Text Cleaning:** Remove HTML tags, punctuation, convert to lowercase, remove stopwords, and possibly do stemming/lemmatization to reduce vocabulary size.\n","* **Vectorization:**\n","  Use **TF-IDF vectorization** or **CountVectorizer** to convert text into numeric features. TF-IDF is great because it weighs down very common words and highlights distinctive words.\n","* **Handling Missing Data:**\n","\n","  * Missing or incomplete emails (empty body, missing subject) can be treated as empty strings or filled with placeholders.\n","  * Ensure vectorizer handles empty inputs gracefully.\n","  * If metadata is missing (like sender info), you could either drop those features or impute reasonable defaults.\n","\n","## 2. **Choosing the Model: SVM vs Naïve Bayes**\n","\n","* **Naïve Bayes:**\n","\n","  * Fast and effective for text classification, especially spam filtering.\n","  * Handles high-dimensional sparse data well (like word counts).\n","  * Robust to noisy features and works well with smaller datasets.\n","\n","* **SVM:**\n","\n","  * Can achieve higher accuracy with proper tuning, especially with kernels like linear or RBF.\n","  * More computationally intensive but often better at complex boundaries.\n","\n","**Recommendation:**\n","Start with **Multinomial Naïve Bayes** as a baseline because it’s simple, fast, and proven effective in spam filtering. If accuracy needs improvement, move to **linear SVM** with TF-IDF features.\n","\n","## 3. **Addressing Class Imbalance**\n","\n","* **Why important?**\n","  Spam is usually a small fraction compared to legitimate emails, so models might get biased toward the majority class.\n","\n","* **Approaches:**\n","\n","  * **Resampling:**\n","\n","    * **Oversampling** the minority class (e.g., using SMOTE)\n","    * **Undersampling** the majority class\n","  * **Class Weights:**\n","\n","    * Many models like SVM and Naïve Bayes accept class weights or priors to penalize misclassification of minority class more heavily.\n","  * **Threshold Tuning:**\n","\n","    * Adjust classification thresholds to improve recall on spam without hurting precision too much.\n","\n","## 4. **Evaluating Performance**\n","\n","* **Metrics to track:**\n","\n","  * **Precision:** How many predicted spam emails are actually spam (important to avoid false alarms).\n","  * **Recall:** How many actual spam emails are detected (important to catch as much spam as possible).\n","  * **F1-score:** Balance between precision and recall.\n","  * **ROC-AUC / PR-AUC:** For overall discrimination ability, especially useful with class imbalance.\n","  * **Confusion Matrix:** To understand types of errors.\n","\n","* **Why precision & recall?**\n","  In spam detection, false positives (legitimate emails marked as spam) can annoy users, while false negatives (spam missed) reduce effectiveness.\n","\n","## 5. **Business Impact**\n","\n","* **Improved User Experience:**\n","  Automatically filtering spam reduces clutter in users’ inboxes, improving satisfaction and productivity.\n","\n","* **Security:**\n","  Effective spam detection helps block phishing, malware, and fraud attempts, protecting company and users.\n","\n","* **Cost Efficiency:**\n","  Reduces manual moderation and support requests related to spam.\n","\n","* **Reputation:**\n","  Maintaining high-quality communication channels preserves trust and brand integrity.\n","\n","### Summary:\n","\n","| Step               | Approach                                                    |\n","| ------------------ | ----------------------------------------------------------- |\n","| Preprocessing      | Clean text, TF-IDF vectorization, handle missing gracefully |\n","| Model Choice       | Start with Multinomial Naïve Bayes; move to SVM if needed   |\n","| Class Imbalance    | Use class weights, resampling, threshold tuning             |\n","| Evaluation Metrics | Precision, Recall, F1-score, ROC-AUC, Confusion Matrix      |\n","| Business Value     | Better UX, security, cost savings, and reputation           |\n","\n","---"],"metadata":{"id":"tlPipi2pPLae"}}]}