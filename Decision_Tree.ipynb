{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **THEORY QUESTIONS**"
      ],
      "metadata": {
        "id": "Nk7irbXaNP70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm that predicts an output by following a series of decision rules based on the values of input features. In classification, it’s used to assign data points into predefined categories.\n",
        "\n",
        "**How it works in classification**:\n",
        "\n",
        "**1. Root node creation** – The algorithm starts with the entire dataset at the root node.\n",
        "\n",
        "**2. Best split selection** – At each node, it evaluates all features and possible split points using an impurity measure such as Gini Impurity or Entropy. The split that results in the most “pure” child nodes is chosen.\n",
        "\n",
        "**3. Recursive splitting** – The dataset is divided into smaller subsets, and this process is repeated for each subset, creating branches of the tree.\n",
        "\n",
        "**4. Stopping criteria** – Splitting stops when a node becomes pure (all samples from one class) or when other conditions are met (e.g., maximum depth, minimum samples per node).\n",
        "\n",
        "**5. Prediction** – To classify a new observation, the model starts at the root and follows the decision rules until it reaches a leaf node. The majority class in that leaf is the prediction.\n",
        "\n",
        "**Example**: If `petal length ≤ 2.45 cm` → classify as Setosa. Else if `petal width ≤ 1.75 cm` → classify as Versicolor. Otherwise → classify as *Virginica*.\n",
        "\n",
        "**Advantages**: simple to understand, interpretable, handles numeric & categorical data.\n",
        "\n",
        "**Limitation**: prone to overfitting if not properly controlled.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "**1. Gini Impurity**\n",
        "\n",
        "* Measures how often a randomly chosen element from the node would be incorrectly classified if it was randomly labeled according to the distribution of labels in the node.\n",
        "* Formula:\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum_{k=1}^{K} p_k^2\n",
        "$$\n",
        "\n",
        "where $p_k$ is the proportion of samples belonging to class $k$ in the node.\n",
        "\n",
        "* **Range:** 0 (pure node) to a maximum value when classes are evenly distributed (e.g., 0.5 for 2 classes at 50%-50%).\n",
        "\n",
        "**2. Entropy**\n",
        "\n",
        "* Comes from information theory; measures the amount of disorder or uncertainty in the node.\n",
        "* Formula:\n",
        "\n",
        "$$\n",
        "Entropy = - \\sum_{k=1}^{K} p_k \\log_2(p_k)\n",
        "$$\n",
        "\n",
        "where $p_k$ is the proportion of samples in class $k$.\n",
        "\n",
        "* **Range:** 0 (pure node) to $\\log_2(K)$ for perfectly mixed classes.\n",
        "\n",
        "**Impact on Decision Tree splits:**\n",
        "\n",
        "* At each node, the algorithm tests possible splits and calculates the weighted average impurity (Gini or Entropy) of the resulting child nodes.\n",
        "* The **best split** is the one with the **largest impurity reduction**:\n",
        "\n",
        "$$\n",
        "\\text{Impurity Reduction} = Impurity_{parent} - \\left( \\frac{n_L}{n_{total}} Impurity_L + \\frac{n_R}{n_{total}} Impurity_R \\right)\n",
        "$$\n",
        "\n",
        "* Gini and Entropy usually select similar splits, but:\n",
        "\n",
        "  * **Gini** is slightly faster to compute and tends to favor the most frequent class.\n",
        "  * **Entropy** is more sensitive to class imbalance and can result in slightly different splits in some cases.\n",
        "\n",
        "  ---\n",
        "\n",
        "### **3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Pre-Pruning (Early Stopping)**\n",
        "\n",
        "* The tree’s growth is **stopped early** based on certain conditions **before** it becomes overly complex.\n",
        "* Common stopping criteria:\n",
        "\n",
        "  * Maximum depth (`max_depth`)\n",
        "  * Minimum samples to split (`min_samples_split`)\n",
        "  * Minimum samples in a leaf (`min_samples_leaf`)\n",
        "  * Minimum impurity decrease\n",
        "* **Practical advantage:** Saves computation time and prevents overfitting by keeping the model simpler from the start.\n",
        "\n",
        "**Post-Pruning (Prune After Full Growth)**\n",
        "\n",
        "* The tree is **grown to its maximum size** (or nearly so), then branches that provide little predictive power are **removed**.\n",
        "* Techniques:\n",
        "\n",
        "  * Reduced Error Pruning (evaluate on validation set and remove unhelpful branches)\n",
        "  * Cost Complexity Pruning (`ccp_alpha` in scikit-learn)\n",
        "* **Practical advantage:** Allows the model to initially capture complex patterns and then simplifies it for better generalization, often leading to improved accuracy on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Information Gain** measures how much **uncertainty** (impurity) is reduced in the target variable after splitting a dataset based on a particular feature.\n",
        "It is most often calculated using **entropy** as the impurity measure.\n",
        "\n",
        "**Formula:**\n",
        "For a split $S$ that divides a parent node $P$ into left ($L$) and right ($R$) child nodes:\n",
        "\n",
        "$$\n",
        "\\text{Information Gain} = Entropy(P) - \\left( \\frac{n_L}{n_P} \\cdot Entropy(L) + \\frac{n_R}{n_P} \\cdot Entropy(R) \\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Entropy(X)$ = impurity of node $X$\n",
        "* $n_L, n_R, n_P$ = number of samples in left child, right child, and parent node.\n",
        "\n",
        "**Why it’s important for choosing the best split:**\n",
        "\n",
        "* At each node, the Decision Tree algorithm evaluates all possible splits and computes their Information Gain.\n",
        "* **Higher Information Gain** means the split makes the child nodes **purer** (more homogeneous) compared to the parent.\n",
        "* The split with the **maximum Information Gain** is chosen because it best separates the classes, leading to a more accurate and efficient tree.\n",
        "\n",
        "**Example:**\n",
        "If splitting on “petal length ≤ 2.45” reduces entropy from **1.0** to an average of **0.2** in the child nodes, the Information Gain is **0.8**, meaning the split significantly improves class purity.\n",
        "\n",
        "---\n",
        "### **5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Common real-world applications:**\n",
        "\n",
        "1. **Medical diagnosis** – Predicting if a patient has a disease based on symptoms and test results.\n",
        "2. **Credit scoring** – Classifying loan applicants as “low risk” or “high risk” based on financial history.\n",
        "3. **Fraud detection** – Identifying suspicious transactions.\n",
        "4. **Customer churn prediction** – Predicting if a customer will stop using a service.\n",
        "5. **Product recommendation** – Suggesting products based on user behavior and preferences.\n",
        "6. **Regression tasks** – Predicting house prices, sales forecasting, etc.\n",
        "\n",
        "**Main advantages:**\n",
        "\n",
        "* **Easy to understand & interpret** – Produces clear, human-readable rules.\n",
        "* **Handles both numerical and categorical data** – Works with mixed data types without complex preprocessing.\n",
        "* **No need for feature scaling** – Normalization or standardization is not required.\n",
        "* **Captures non-linear relationships** – Can model complex decision boundaries.\n",
        "* **Feature importance** – Identifies which variables are most influential.\n",
        "\n",
        "**Main limitations:**\n",
        "\n",
        "* **Overfitting** – Fully grown trees can memorize the training data and perform poorly on unseen data.\n",
        "* **High variance** – Small changes in data can produce very different trees.\n",
        "* **Bias toward features with many levels** – Features with more unique values can dominate splits.\n",
        "* **Less accurate than ensembles** – Often outperformed by Random Forests or Gradient Boosting on complex problems.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qvsAcneENh5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "8kbo3aWmRsFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "**Answer**:\n"
      ],
      "metadata": {
        "id": "slOq6DVoSPJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and test sets (stratified to maintain class proportions)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Output\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Feature importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAD_JQLMChVw",
        "outputId": "5019e2e7-9ea3-4ef2-d2a7-48429351a94c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333\n",
            "Feature importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0286\n",
            "  petal length (cm): 0.5412\n",
            "  petal width (cm): 0.4303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with *`max_depth=3`* and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "Q7u90MQS6O96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Fully-grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Tree with max_depth=3\n",
        "clf_md3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "# Output\n",
        "print(f\"Fully-grown tree accuracy: {acc_full:.4f}\")\n",
        "print(f\"Max depth=3 tree accuracy: {acc_md3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOhuc8561Hg",
        "outputId": "a6b8e483-1992-49d9-95b4-5846c5b44489"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 0.9333\n",
            "Max depth=3 tree accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Write a Python program to:**\n",
        "● Load the California Housing dataset from sklearn\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "ETaJe7uY7CXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error on test data: {mse:.4f}\\n\")\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\" - {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cSorXq_7OIa",
        "outputId": "5b6d4e80-e6d9-4977-ca9f-07de3ceaccbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on test data: 0.4952\n",
            "\n",
            "Feature Importances:\n",
            " - MedInc: 0.5285\n",
            " - HouseAge: 0.0519\n",
            " - AveRooms: 0.0530\n",
            " - AveBedrms: 0.0287\n",
            " - Population: 0.0305\n",
            " - AveOccup: 0.1308\n",
            " - Latitude: 0.0937\n",
            " - Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s *`max_depth`* and *`min_samples_split`* using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "y2jPmhcv7fiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up the grid of parameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearch to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate the best estimator on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Model accuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nqnqhX_71LG",
        "outputId": "1fc0eeb3-3c42-46c6-b891-138211b97c7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:**\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### 1. **Handle Missing Values**\n",
        "\n",
        "* **Understand the missingness:**\n",
        "  First, analyze *how* data is missing. Is it random, or does it follow a pattern? This affects how you handle it.\n",
        "\n",
        "* **Imputation:**\n",
        "\n",
        "  * For **numerical features**, you could fill missing values with mean, median, or use more advanced methods like K-Nearest Neighbors imputation.\n",
        "  * For **categorical features**, you might fill missing with the mode (most frequent value) or create a special category like `\"Unknown\"`.\n",
        "  * If missing values are too prevalent or critical, consider dropping those features or samples carefully.\n",
        "\n",
        "* **Why it matters:**\n",
        "  Models can’t handle missing data directly, so cleaning this up ensures your model sees complete, reliable inputs.\n",
        "\n",
        "### 2. **Encode Categorical Features**\n",
        "\n",
        "* **Identify categorical variables:**\n",
        "  This could be patient gender, blood type, or any non-numeric info.\n",
        "\n",
        "* **Encoding methods:**\n",
        "\n",
        "  * For **nominal categories** without order (e.g., blood type), use **One-Hot Encoding**.\n",
        "  * For **ordinal categories** (e.g., disease severity: mild, moderate, severe), use **Label Encoding** or map them to meaningful numeric scales.\n",
        "\n",
        "* **Why it matters:**\n",
        "  Machine learning models, including Decision Trees, require numeric input, so encoding transforms your data into a digestible form.\n",
        "\n",
        "### 3. **Train a Decision Tree Model**\n",
        "\n",
        "* **Split the data:**\n",
        "  Use an 80-20 or 70-30 split between training and test sets, or use cross-validation to ensure your results generalize.\n",
        "\n",
        "* **Initialize the model:**\n",
        "  Start with a default Decision Tree classifier.\n",
        "\n",
        "* **Train on processed data:**\n",
        "  Fit the model on your training data.\n",
        "\n",
        "* **Why Decision Trees:**\n",
        "  They handle mixed data types well, are interpretable (important in healthcare), and can capture nonlinear patterns.\n",
        "\n",
        "### 4. **Tune Hyperparameters**\n",
        "\n",
        "* **Key hyperparameters to tune:**\n",
        "\n",
        "  * `max_depth`: controls tree complexity, balancing underfitting and overfitting.\n",
        "  * `min_samples_split` and `min_samples_leaf`: control how many samples needed to split or be a leaf node, affecting generalization.\n",
        "  * `max_features`: number of features to consider at each split.\n",
        "\n",
        "* **Use GridSearchCV or RandomizedSearchCV:**\n",
        "  Explore combinations systematically with cross-validation to find the sweet spot.\n",
        "\n",
        "* **Why tuning matters:**\n",
        "  Proper tuning prevents overfitting or underfitting, improving the model’s predictive power on unseen data.\n",
        "\n",
        "### 5. **Evaluate Performance**\n",
        "\n",
        "* **Metrics:**\n",
        "\n",
        "  * For classification, consider **Accuracy**, **Precision**, **Recall**, **F1-score**, and **ROC-AUC**.\n",
        "  * In healthcare, **Recall** (sensitivity) is often critical — you want to catch as many patients with the disease as possible, even at the cost of some false positives.\n",
        "\n",
        "* **Validation:**\n",
        "  Use a separate test set or cross-validation to ensure your model performs reliably.\n",
        "\n",
        "* **Interpretability:**\n",
        "  Use feature importance and decision tree visualization to explain model decisions to clinicians and stakeholders.\n",
        "\n",
        "### **Business Value of This Model**\n",
        "\n",
        "* **Early detection:**\n",
        "  Predicting disease early means patients can receive timely treatment, improving outcomes and reducing healthcare costs.\n",
        "\n",
        "* **Resource optimization:**\n",
        "  Helps healthcare providers prioritize high-risk patients for screening or intervention, making better use of limited resources.\n",
        "\n",
        "* **Personalized care:**\n",
        "  Tailors monitoring and care plans based on individual risk, improving patient satisfaction and effectiveness.\n",
        "\n",
        "* **Data-driven decisions:**\n",
        "  Provides actionable insights backed by data, enabling the company to develop better products, policies, or outreach programs.\n",
        "\n",
        "* **Trust and transparency:**\n",
        "  Decision Trees’ interpretability supports building trust with clinicians, regulators, and patients, crucial in healthcare.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wbN3xRAd8Lm_"
      }
    }
  ]
}